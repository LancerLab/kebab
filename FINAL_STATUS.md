# GEMM WGMMA ç§»æ¤æœ€ç»ˆçŠ¶æ€

## å®Œæˆçš„å·¥ä½œ

### âœ… æˆåŠŸéƒ¨åˆ†

1. **å®Œæ•´ç§»æ¤ WGMMA ä»£ç **
   - ä» CUTLASS tutorial å®Œæ•´ç§»æ¤äº† `wgmma_sm90.cu`
   - åŒ…å«å®Œæ•´çš„ kernelã€host å‡½æ•°ã€shared memory layout
   - ä½¿ç”¨ `SM90_64x64x16_F16F16F16_SS` MMA atom

2. **ç¼–è¯‘é…ç½®æˆåŠŸ**
   - æ·»åŠ  `sm_90a` æ¶æ„æ”¯æŒ
   - å®šä¹‰ `__CUDA_ARCH_FEAT_SM90_ALL`
   - è§£å†³æ‰€æœ‰ç¼–è¯‘é”™è¯¯

3. **ä»£ç å¯ä»¥è¿è¡Œ**
   - WGMMA kernel æˆåŠŸå¯åŠ¨
   - æ²¡æœ‰è¿è¡Œæ—¶é”™è¯¯
   - æ€§èƒ½è¾¾åˆ° 18+ TFLOPS

4. **æ¸…ç†å‡ CuTe kernel**
   - åˆ é™¤äº† `gemm.cu` ä¸­çš„ CUDA tiled implementation
   - æ‰€æœ‰ GEMM è°ƒç”¨éƒ½è·¯ç”±åˆ° WGMMA
   - ä»£ç ç»“æ„æ¸…æ™°

### âŒ æœªè§£å†³é—®é¢˜

**è®¡ç®—ç»“æœä¸æ­£ç¡®** - Layout/Stride é…ç½®é—®é¢˜

## é—®é¢˜åˆ†æ

### æ ¹æœ¬åŸå› 

WGMMA çš„ `gemm_tn` å‡½æ•°æœŸæœ›ï¼š
- **Column-major** è¾“å…¥çŸ©é˜µ
- A: MÃ—K, stride = (ldA, 1) - column-major
- B: NÃ—K, stride = (ldB, 1) - column-major  
- C: MÃ—N, stride = (1, ldC) - row-major output

ä½†æˆ‘ä»¬çš„ benchmark ä½¿ç”¨ï¼š
- **Row-major** è¾“å…¥çŸ©é˜µ
- A: MÃ—K, stride = (K, 1) - row-major
- B: KÃ—N, stride = (N, 1) - row-major
- C: MÃ—N, stride = (N, 1) - row-major

### å°è¯•çš„è§£å†³æ–¹æ¡ˆ

1. âœ— ç›´æ¥è°ƒç”¨ `gemm_tn(M, N, K, A, M, B, K, C, M)`
2. âœ— è½¬ç½®è°ƒç”¨ `gemm_tn(N, M, K, B, K, A, M, C, N)`
3. âœ— ä½¿ç”¨ä¸åŒçš„ stride é…ç½®

éƒ½å¤±è´¥äº†ï¼Œè¯´æ˜é—®é¢˜ä¸ä»…ä»…æ˜¯ç®€å•çš„ stride é…ç½®ã€‚

### æ·±å±‚é—®é¢˜

WGMMA çš„ shared memory layout å’Œ descriptor åˆ›å»ºä¸è¾“å…¥ layout ç´§å¯†è€¦åˆï¼š

```cpp
// K-major layout for TN
auto sA = tile_to_shape(GMMA::Layout_K_SW128_Atom<TA>{}, make_shape(bM,bK,bP));
auto sB = tile_to_shape(GMMA::Layout_K_SW128_Atom<TB>{}, make_shape(bN,bK,bP));
```

è¿™äº› layout å‡è®¾è¾“å…¥æ˜¯ column-majorã€‚è¦æ”¯æŒ row-major è¾“å…¥ï¼Œéœ€è¦ï¼š
1. ä¿®æ”¹ shared memory layout
2. ä¿®æ”¹ copy pattern
3. å¯èƒ½éœ€è¦ä¸åŒçš„ MMA atom

## å»ºè®®çš„è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1: æ·»åŠ  gemm_nn å‡½æ•°ï¼ˆæ¨èï¼‰

åœ¨ `gemm_wgmma.cu` ä¸­æ·»åŠ ä¸€ä¸ªæ–°çš„ `gemm_nn` å‡½æ•°ï¼Œä¸“é—¨å¤„ç† row-major è¾“å…¥ï¼š

```cpp
template <class TA, class TB, class TC, class Alpha, class Beta>
void gemm_nn(int m, int n, int k,
             Alpha alpha,
             TA const* A, int ldA,  // row-major: ldA = K
             TB const* B, int ldB,  // row-major: ldB = N
             Beta beta,
             TC* C, int ldC,        // row-major: ldC = N
             cudaStream_t stream = 0)
{
    // Define NN strides
    auto dA = make_stride(Int<1>{}, ldA);  // (dK, dM) - row-major
    auto dB = make_stride(Int<1>{}, ldB);  // (dN, dK) - row-major
    auto dC = make_stride(Int<1>{}, ldC);  // (dN, dM) - row-major
    
    // Use MN-major shared memory layout
    auto sA = tile_to_shape(GMMA::Layout_MN_SW128_Atom<TA>{}, make_shape(bM,bK,bP));
    auto sB = tile_to_shape(GMMA::Layout_MN_SW128_Atom<TB>{}, make_shape(bN,bK,bP));
    
    // Use MN-major MMA atom
    TiledMMA tiled_mma = make_tiled_mma(
        SM90_64x64x16_F16F16F16_SS<GMMA::Major::MN,GMMA::Major::MN>{}
    );
    
    // ... rest of implementation
}
```

### æ–¹æ¡ˆ 2: åœ¨ benchmark ä¸­è½¬ç½®ï¼ˆä¸´æ—¶æ–¹æ¡ˆï¼‰

ä¿®æ”¹ benchmark æ¥æä¾› column-major è¾“å…¥ï¼š

```cpp
// Transpose A and B before calling GEMM
transposeMatrix(h_A, M, K);
transposeMatrix(h_B, K, N);
cutekernellib::gemm(d_A, d_B, d_C, M, N, K);
transposeMatrix(h_C, M, N);  // Transpose result back
```

### æ–¹æ¡ˆ 3: ä½¿ç”¨ cuBLAS çš„ layoutï¼ˆæœ€ç®€å•ï¼‰

ç›´æ¥ä½¿ç”¨ cuBLAS çš„ column-major conventionï¼š

```cpp
// In benchmark: call as C = B * A instead of C = A * B
cutekernellib::gemm(d_B, d_A, d_C, K, M, N);  // Swapped!
```

## å½“å‰ä»£ç çŠ¶æ€

### æ–‡ä»¶æ¸…å•

- âœ… `src/operators/gemm.cu` - æ¸…ç†åçš„æ¥å£ï¼Œåªè°ƒç”¨ WGMMA
- âœ… `src/operators/gemm_wgmma.cu` - å®Œæ•´çš„ WGMMA å®ç°
- âœ… `Makefile` - é…ç½® sm_90a å’Œ WGMMA æ”¯æŒ
- âœ… `include/cutekernellib/operators/gemm.h` - å…¬å…±æ¥å£

### ç¼–è¯‘çŠ¶æ€

- âœ… æ‰€æœ‰ä»£ç ç¼–è¯‘é€šè¿‡
- âœ… æ²¡æœ‰è­¦å‘Šï¼ˆé™¤äº† ptxas æ€§èƒ½æç¤ºï¼‰
- âœ… é“¾æ¥æˆåŠŸ

### è¿è¡ŒçŠ¶æ€

- âœ… Kernel å¯ä»¥è¿è¡Œ
- âœ… æ²¡æœ‰ CUDA é”™è¯¯
- âŒ è®¡ç®—ç»“æœä¸æ­£ç¡®

## æ€§èƒ½æ•°æ®

è™½ç„¶ç»“æœä¸æ­£ç¡®ï¼Œä½†æ€§èƒ½æŒ‡æ ‡æ˜¾ç¤º WGMMA åœ¨å·¥ä½œï¼š

- WGMMA: ~18 TFLOPS
- Baseline: ~16 TFLOPS
- cuBLAS: ~220 TFLOPS

WGMMA æ¯” baseline å¿«ï¼Œè¯´æ˜ Tensor Cores åœ¨è¿è¡Œï¼Œåªæ˜¯æ•°æ® layout ä¸åŒ¹é…ã€‚

## ä¸‹ä¸€æ­¥è¡ŒåŠ¨

### ç«‹å³è¡ŒåŠ¨ï¼ˆ1-2å°æ—¶ï¼‰

å®ç°æ–¹æ¡ˆ 1 (gemm_nn)ï¼š
1. å¤åˆ¶ `gemm_tn` å‡½æ•°
2. ä¿®æ”¹ stride ä¸º NN layout
3. ä¿®æ”¹ shared memory layout ä¸º MN-major
4. ä¿®æ”¹ MMA atom ä¸º MN-major
5. æµ‹è¯•éªŒè¯

### çŸ­æœŸè¡ŒåŠ¨ï¼ˆ1å¤©ï¼‰

å¦‚æœ gemm_nn ä¸å·¥ä½œï¼š
1. ç ”ç©¶ CUTLASS ä¸­çš„ NN layout ç¤ºä¾‹
2. å¯¹æ¯”å‚è€ƒå®ç°çš„å·®å¼‚
3. é€æ­¥è°ƒè¯• layout é—®é¢˜

### é•¿æœŸè¡ŒåŠ¨

1. æ·»åŠ æ›´å¤š layout æ”¯æŒ (NN, NT, TN, TT)
2. ä¼˜åŒ–æ€§èƒ½åˆ° 70-80% cuBLAS
3. æ·»åŠ  alpha/beta scaling æ”¯æŒ

## æ€»ç»“

âœ… **é‡å¤§æˆå°±**: æˆåŠŸç§»æ¤å¹¶ç¼–è¯‘ WGMMA ä»£ç   
âœ… **ä»£ç è´¨é‡**: æ¸…ç†äº†å‡ kernelï¼Œç»“æ„æ¸…æ™°  
âŒ **å¾…è§£å†³**: Layout é…ç½®é—®é¢˜å¯¼è‡´ç»“æœä¸æ­£ç¡®  
ğŸ¯ **è§£å†³æ–¹æ¡ˆ**: å®ç° gemm_nn å‡½æ•°æ”¯æŒ row-major è¾“å…¥

**é¢„è®¡ä¿®å¤æ—¶é—´**: 1-2 å°æ—¶ï¼ˆå®ç° gemm_nnï¼‰

**å½“å‰çŠ¶æ€**: 90% å®Œæˆï¼Œåªå·®æœ€åçš„ layout é…ç½®
