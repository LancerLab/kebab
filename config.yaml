# CuTeKernelLib Configuration File

# Build Configuration
build:
  mode: release              # debug | release
  optimization: O3           # O0 | O1 | O2 | O3
  cuda_arch: auto            # auto | sm_80 | sm_86 | sm_89 | sm_90

# Benchmark Configuration
benchmark:
  warmup_runs: 20
  measurement_runs: 100
  batch_sizes: [256, 512, 1024, 2048, 4096]
  data_types: [float32, float16]

# Profiling Configuration
profiling:
  ncu_metrics:
    - sm__throughput.avg.pct_of_peak_sustained_elapsed
    - dram__throughput.avg.pct_of_peak_sustained_elapsed
    - sm__sass_thread_inst_executed_op_fadd_pred_on.sum
    - sm__sass_thread_inst_executed_op_fmul_pred_on.sum
    - sm__sass_thread_inst_executed_op_ffma_pred_on.sum
    - smsp__sass_average_data_bytes_per_sector_mem_global_op_ld.pct
    - smsp__sass_average_data_bytes_per_sector_mem_global_op_st.pct
    - sm__inst_executed_pipe_tensor.sum
  sections:
    - SpeedOfLight
    - MemoryWorkloadAnalysis
    - ComputeWorkloadAnalysis
    - Occupancy

# Operator Configuration
operators:
  elementwise_add:
    enabled: true
    # GPU device ID to run on (0-based index, -1 for auto-select)
    # Use nvidia-smi to see available GPUs
    gpu_id: 0
    sizes: [256, 512, 1024, 2048, 4096]
    # Implementation: cuda (baseline) | cute (CuTe implementation)
    impl: cuda
    # Initialization method: <lhs>-<rhs>
    # Options: one, rand, range, row, col
    # - one: all elements are 1.0
    # - rand: random values in [-1, 1]
    # - range: sequential values [0, 1, 2, ...]
    # - row: value equals row index
    # - col: value equals column index
    init_method: rand-rand
    
  gemm:
    enabled: true
    # GPU device ID to run on (0-based index, -1 for auto-select)
    gpu_id: 0
    # Implementation: cuda (baseline) | cute (CuTe/WGMMA implementation)
    impl: cuda
    # Version: kernel variant ID
    # cuda version 1: Baseline (naive kernels)
    # cuda version 2: WGMMA + TMA (based on fast.cu kernel 2)
    # cute version 1: WGMMA without TMA
    # cute version 2: WGMMA with TMA
    # cute version 3: Optimized multi-stage pipeline with TMA
    version: 12
    # Initialization method for matrices A and B
    # Format: <A_init>-<B_init>
    # Options: one, rand, range, row, col
    init_method: rand-rand
    # Matrix sizes for benchmarking (M=N=K for square matrices)
    matrix_sizes: [256, 512, 1024, 2048, 4096, 8192, 16384]
    # Operation modes: specify storage format of LHS and RHS
    # Format: <LHS_format><RHS_format>
    # - R: Row-major storage
    # - C: Column-major storage
    # Examples:
    # - RR: Both A and B are row-major (default, most common)
    # - RC: A is row-major, B is column-major
    # - CR: A is column-major, B is row-major
    # - CC: Both A and B are column-major
    modes: [RC]
    # Precision: float32, float16, bfloat16
    # Note: bfloat16 is only supported by CUDA GEMM kernels (not CuTe)
    precisions: [bfloat16]
    # Tile sizes (for tuning): [M, N, K]
    # IMPORTANT: K dimension MUST be 64 for GMMA Layout_K_SW128_Atom compatibility
    # Supported configurations:
    #   [128, 128, 64] - Default, balanced
    #   [256, 128, 64] - Large M dimension
    #   [128, 256, 64] - Large N dimension
    # Note: Other K values (e.g., 32) are NOT supported and will fall back to default
    tile_sizes: [128, 128, 64]
    verbose: false
    
  fft:
    enabled: false
    gpu_id: 0
    sizes: [1024, 2048, 4096, 8192]
    
  conv2d:
    enabled: false
    gpu_id: 0
    channels: [32, 64, 128]
    kernel_sizes: [3, 5, 7]
    
  reduction:
    enabled: false
    gpu_id: 0
    sizes: [1024, 2048, 4096, 8192]

